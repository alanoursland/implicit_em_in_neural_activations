# Candidate Competitive Activations

## The Goal

We want an activation function f such that when a layer computes a = f(Wx + b), the backward pass distributes gradients competitively among units. Units that "claim" an input should receive most of the gradient. Units that don't claim it should receive little.

The gradient structure we want resembles softmax: cross-unit terms in the Jacobian that create zero-sum dynamics. But we may also want properties softmax lacks: preserved magnitude, computational efficiency, compatibility with deep stacking.

## Baseline: ReLU

```
a = relu(z) = max(0, z)
```

**Forward:** Each unit outputs its pre-activation if positive, else zero.

**Jacobian:**
```
âˆ‚aáµ¢/âˆ‚zâ±¼ = Î´áµ¢â±¼ Â· ðŸ™[záµ¢ > 0]
```

Diagonal. No cross-unit terms. Units are independent.

**Gradient flow:** Each unit receives gradient scaled by whether it was active. Dead units (always inactive) receive no gradient. But active units don't competeâ€”all active units receive their full gradient signal.

**Competition:** None.

**Expressiveness:** High. K units can create up to 2^K linear regions (activation patterns). Combinatorial capacity.

**Magnitude:** Preserved for positive values. Lost (zeroed) for negative values.

ReLU is the baseline against which competitive activations must be measured. Any replacement must offer benefits that justify departing from ReLU's simplicity and expressiveness.

---

## Candidate 1: Softmax

```
a = softmax(z), where aáµ¢ = exp(záµ¢) / Î£â‚– exp(zâ‚–)
```

**Forward:** Outputs a probability distribution over units. Sum to one. All positive.

**Jacobian:**
```
âˆ‚aáµ¢/âˆ‚zâ±¼ = aáµ¢(Î´áµ¢â±¼ âˆ’ aâ±¼)
```

Off-diagonal terms: âˆ’aáµ¢aâ±¼ < 0. Raising zâ±¼ lowers aáµ¢ for i â‰  j.

**Gradient flow:** If unit j dominates (aâ±¼ â‰ˆ 1), it receives most gradient. Other units receive gradient proportional to their (small) responsibility, pushing them to differentiate.

**Competition:** Strong. Zero-sum by construction.

**Expressiveness:** Low. Output is on the Kâˆ’1 simplex. Effectively encodes "which unit wins" with soft interpolation. K effective patterns, not 2^K.

**Magnitude:** Lost entirely. Only relative ranking matters. z = [10, 5, 1] and z = [100, 50, 10] produce the same output.

**Assessment:** Maximum competition, minimum expressiveness. Too restrictive for hidden layers. The simplex constraint impoverishes the representation.

---

## Candidate 2: Log-Softmax

```
a = log_softmax(z) = z âˆ’ logsumexp(z)
```

Equivalently: aáµ¢ = záµ¢ âˆ’ log Î£â‚– exp(zâ‚–)

**Forward:** Subtracts a shared value (the LSE) from all units. Output can be any real number. Not normalized to simplex.

**Jacobian:**
```
âˆ‚aáµ¢/âˆ‚zâ±¼ = Î´áµ¢â±¼ âˆ’ softmax(z)â±¼ = Î´áµ¢â±¼ âˆ’ râ±¼
```

Diagonal terms: 1 âˆ’ râ±¼. Off-diagonal terms: âˆ’râ±¼.

**Gradient flow:** The âˆ’râ±¼ term appears in all rows. The dominant unit (high râ±¼) suppresses gradient to all units including itself. Competition exists but is mediated through subtraction rather than normalization.

**Competition:** Moderate. All units are pulled toward the mean log-probability. Dominant units pull others down; weak units pull others up.

**Expressiveness:** Higher than softmax. Output is not on simplex. Relative magnitudes preserved in the sense that aáµ¢ âˆ’ aâ±¼ = záµ¢ âˆ’ zâ±¼. Differences unchanged.

**Magnitude:** Partially preserved. Absolute scale lost (shifted by LSE), but relative structure intact.

**Assessment:** A middle ground. Competition via shared subtraction. More expressive than softmax. But the constant shift may interact strangely with downstream layers. Resembles a residual connection in structure.

---

## Candidate 3: Softmax-Weighted Pre-activation

```
a = z âŠ™ softmax(z)
```

Element-wise product of pre-activation and responsibility.

**Forward:** Each unit outputs its pre-activation scaled by its responsibility. Winners (high z, high r) get large output. Losers (low r) are suppressed regardless of z magnitude.

**Jacobian:**
Let r = softmax(z).
```
âˆ‚aáµ¢/âˆ‚záµ¢ = ráµ¢ + záµ¢ Â· ráµ¢(1 âˆ’ ráµ¢) = ráµ¢(1 + záµ¢(1 âˆ’ ráµ¢))
âˆ‚aáµ¢/âˆ‚zâ±¼ = záµ¢ Â· ráµ¢ Â· (âˆ’râ±¼) = âˆ’záµ¢ráµ¢râ±¼ for i â‰  j
```

**Gradient flow:** Off-diagonal terms are âˆ’záµ¢ráµ¢râ±¼. Sign depends on záµ¢. For positive pre-activations, competition exists. Magnitude of pre-activation amplifies competitive effect.

**Competition:** Present but conditional. Strength depends on magnitudes. Zero pre-activation means no competition from that unit.

**Expressiveness:** Moderate. Not constrained to simplex. Magnitudes partially preservedâ€”scaled by responsibility, not discarded.

**Magnitude:** Transformed. Large z with large r â†’ large output. Large z with small r â†’ suppressed. The scaling is multiplicative.

**Assessment:** Interesting hybrid. Competition exists. Magnitude influences result. But the multiplicative interaction may cause instabilityâ€”large z amplifies both signal and competitive pressure.

---

## Candidate 4: Softmax Plus Pre-activation

```
a = z + softmax(z)
```

Additive combination.

**Forward:** Original pre-activation plus responsibility. Output unbounded.

**Jacobian:**
```
âˆ‚aáµ¢/âˆ‚zâ±¼ = Î´áµ¢â±¼ + ráµ¢(Î´áµ¢â±¼ âˆ’ râ±¼)
        = Î´áµ¢â±¼(1 + ráµ¢(1 âˆ’ ráµ¢)) âˆ’ ráµ¢râ±¼(1 âˆ’ Î´áµ¢â±¼)
```

Diagonal: 1 + ráµ¢(1 âˆ’ ráµ¢) > 1. Off-diagonal: âˆ’ráµ¢râ±¼ < 0.

**Gradient flow:** Diagonal terms boosted. Off-diagonal terms negative. Competition exists via the softmax component.

**Competition:** Weak. The identity component (âˆ‚z/âˆ‚z = I) dominates. Softmax competition is additive, not multiplicative.

**Expressiveness:** High. The z term preserves full pre-activation information. Softmax term adds soft assignment signal.

**Magnitude:** Fully preserved in the z component.

**Assessment:** Minimal modification to ReLU-like behavior. Competition is present but may be too weak to force specialization. The softmax term is a perturbation, not a restructuring.

---

## Candidate 5: Grouped Softmax

```
Partition units into G groups of size K/G.
Within each group: a_g = softmax(z_g)
Across groups: independent.
```

**Forward:** Competition within groups. Independence across groups.

**Jacobian:** Block diagonal. Each block is a softmax Jacobian. No cross-group terms.

**Gradient flow:** Units compete with their group-mates, not with all units.

**Competition:** Local. Strong within groups. None across groups.

**Expressiveness:** Intermediate. G groups with K/G options each gives (K/G)^G effective patterns. More than pure softmax (K), less than ReLU (2^K).

**Magnitude:** Lost within groups (softmax). Group outputs could be concatenated.

**Assessment:** Tunable competition/expressiveness tradeoff via group size. Group size 2 gives pairwise competition. Group size K gives full softmax. May be useful for controlled experiments.

---

## Candidate 6: Sparsemax

```
a = sparsemax(z) = argmin_{p âˆˆ Î”} ||p âˆ’ z||Â²
```

Euclidean projection onto the simplex. Unlike softmax, produces exact zeros.

**Forward:** Sparse probability distribution. Some units get exactly zero weight. Others share the remaining mass.

**Jacobian:** Piecewise linear. Non-zero only for the "support" (active units). Within support, resembles softmax Jacobian but renormalized.

**Gradient flow:** Inactive units receive exactly zero gradient. Active units compete among themselves.

**Competition:** Strong among active units. Inactive units are hard-zeroed like ReLU.

**Expressiveness:** Between softmax and ReLU. Sparse like ReLU. Normalized like softmax.

**Magnitude:** Lost. Output is on simplex.

**Assessment:** Combines sparsity with competition. May get benefits of both. But still loses magnitude. And introduces non-differentiability at the boundary of the support.

---

## Candidate 7: Temperature-Scaled Softmax

```
a = softmax(z / Ï„)
```

Temperature Ï„ controls sharpness.

**Forward:** High Ï„ â†’ uniform (weak competition). Low Ï„ â†’ winner-take-all (strong competition).

**Jacobian:** Same structure as softmax, but responsibilities r are computed at temperature Ï„.

**Gradient flow:** Ï„ modulates competition strength. Learnable Ï„ could adapt during training.

**Competition:** Tunable. Ï„ â†’ 0 gives hard assignment. Ï„ â†’ âˆž gives no competition.

**Expressiveness:** Still constrained to simplex.

**Magnitude:** Still lost.

**Assessment:** Useful knob for controlling competition strength. Doesn't solve magnitude problem. Could be combined with other approaches.

---

## Candidate 8: Softmax Gate on ReLU

```
a = relu(z) âŠ™ softmax(z)
```

ReLU for magnitude and sparsity. Softmax for competition.

**Forward:** Active units (z > 0) output their value scaled by responsibility. Inactive units (z â‰¤ 0) output zero.

**Jacobian:** Complex interaction of ReLU indicator and softmax derivatives.

For záµ¢ > 0, zâ±¼ > 0:
```
âˆ‚aáµ¢/âˆ‚záµ¢ = ráµ¢ + záµ¢ráµ¢(1 âˆ’ ráµ¢)
âˆ‚aáµ¢/âˆ‚zâ±¼ = âˆ’záµ¢ráµ¢râ±¼
```

For záµ¢ â‰¤ 0: âˆ‚aáµ¢/âˆ‚zâ±¼ = 0 for all j.

**Gradient flow:** Inactive units are dead (like ReLU). Active units compete (like softmax-weighted).

**Competition:** Among active units only.

**Expressiveness:** Sparsity from ReLU. Competition among survivors.

**Magnitude:** Preserved for active units, scaled by responsibility.

**Assessment:** Hybrid that may capture benefits of both. Dead units remain deadâ€”no competition can revive them. This could be a feature (sparse) or bug (capacity loss).

---

## Summary Table

| Activation | Competition | Magnitude | Expressiveness | Gradient Complexity |
|------------|-------------|-----------|----------------|---------------------|
| ReLU | None | Partial (â‰¥0) | High (2^K) | Simple |
| Softmax | Strong | Lost | Low (K) | Moderate |
| Log-softmax | Moderate | Relative | Moderate | Simple |
| z âŠ™ softmax(z) | Conditional | Scaled | Moderate | Moderate |
| z + softmax(z) | Weak | Full | High | Moderate |
| Grouped softmax | Local | Lost in groups | Tunable | Block diagonal |
| Sparsemax | Strong + sparse | Lost | Moderate | Piecewise |
| Temp softmax | Tunable | Lost | Low | Moderate |
| relu âŠ™ softmax | Among active | Scaled | Moderate | Complex |

---

## What's Missing

None of these candidates clearly dominate. The core tension:

- **Competition requires normalization.** Normalization loses magnitude or absolute scale.
- **Magnitude preservation avoids normalization.** Without normalization, no competition.

Possible resolutions:

1. **Accept magnitude loss.** Maybe downstream layers don't need magnitude. Test empirically.

2. **Parallel paths.** Compute competitive and non-competitive activations separately. Concatenate. Let downstream layers use both signals.

3. **Normalization in a subspace.** Compete over direction, preserve magnitude separately. Like: a = ||z|| Â· softmax(z/||z||). Magnitude in the norm, competition in the direction.

4. **Learned tradeoff.** Let the network learn how much competition to apply. Gating between ReLU and softmax paths.

5. **New activation not yet considered.** The right answer may not be on this list.

---

## Experimental Priority

If testing, order by likely insight:

1. **Softmax** â€” Maximum competition baseline. Does it specialize? Does it train at all?

2. **Log-softmax** â€” Moderate competition, some magnitude. Practical middle ground?

3. **z âŠ™ softmax(z)** â€” Novel hybrid. Competition with magnitude. Stability unknown.

4. **Grouped softmax** â€” Tunable competition. Can we find the right group size?

5. **Others** â€” As needed based on results.