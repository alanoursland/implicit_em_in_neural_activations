\section{Introduction}
\label{sec:introduction}

Deep learning models are typically designed through intuition and experimentation, with theory following to explain what works. This paper inverts that order. We begin from a theoretical result, implicit expectation--maximization (EM) in neural networks, derive a model from its requirements, and test whether the prescription succeeds. The theory makes specific predictions: what architecture is required, what objective to use, and what failure modes will occur without each component. We build exactly what the theory specifies and test each prediction. The resulting model works, not because it was optimized for benchmarks, but because principled derivation yields a coherent design.

\subsection{Implicit EM Theory}
\label{sec:implicit-em-theory}

Prior work shows that gradient descent on log-sum-exp (LSE) loss results in implicit EM during backpropagation \citep{oursland2025implicit}. This occurs because the gradient with respect to each component output equals its responsibility, the posterior probability that the component explains the input. This identity is exact. The forward pass computes responsibilities via a softmax; the backward pass returns those same quantities as gradients; parameter updates therefore play the role of the M-step.

Prior work interprets neural network outputs as distances from learned prototypes, where lower values indicate better matches \citep{oursland2024mahalanobis}. Under this view, softmax is an assignment mechanism. Responsibilities sum to one across competing components, distributing each input among them. \Cref{sec:theory} develops this in detail.

The theory also exposes a limitation. Classical mixture models include a log-determinant term that prevents components from collapsing to points. Neural LSE objectives typically lack an analogous constraint. When implicit EM is present without corresponding safeguards, representations degenerate. We test this prediction directly.

\subsection{The Question}
\label{sec:question}

Theories can explain or they can prescribe. An explanatory theory redescribes existing models in new terms. This aids understanding but not necessarily building. A prescriptive theory specifies what to build from first principles and risks falsification if that specification fails.

This paper investigates if implicit EM theory can be prescriptive. The question is not whether it can be used to improve sparse autoencoders, which would treat the theory as a source of heuristics layered onto existing designs. The question is whether the theory itself specifies a working model. If we build exactly what implicit EM requires, do we obtain a functioning system? Do the predicted failure modes appear when required components are removed, and do the predicted behaviors emerge when the full system is trained?

A positive answer would establish implicit EM as a foundation for principled model design. A negative answer would confine it to post-hoc interpretation.

\subsection{This Paper}
\label{sec:this-paper}

We construct a model using implicit EM theory. The architecture is minimal, consisting of a single linear layer followed by ReLU. The LSE loss provides EM dynamics. InfoMax regularization provides volume control. Variance penalties prevent collapse. Decorrelation penalties prevent redundancy. These are neural equivalents of the log-determinant term in Gaussian mixture models. We add nothing for empirical convenience and omit nothing the theory requires.

\begin{equation}
\label{eq:objective-intro}
L = L_{\text{LSE}} + L_{\text{var}} + L_{\text{decorr}}
\end{equation}

The result is a decoder-free sparse autoencoder, or equivalently, a neural mixture model. There is no reconstruction loss, no L1 sparsity penalty, and no decoder. Sparse, interpretable features emerge through competition alone: components specialize because the objective rewards specialization.

Experiments test the theory's predictions directly. We verify the gradient--responsibility identity exactly. We show that LSE alone collapses, that variance prevents dead components, and that decorrelation prevents redundancy. The learned features are mixture components, digit prototypes rather than dictionary elements. Training dynamics exhibit EM-like properties. SGD is learning-rate insensitive and adaptive optimizers offer no advantage.

\subsection{Contribution}
\label{sec:contribution}

This paper makes four contributions.

\paragraph{Implicit EM theory is generative.} The theory specifies requirements that constrain model design. We derived an architecture and objective from those requirements. The resulting unsupervised model works, and its behavior confirms theoretical predictions.

\paragraph{Each theoretical prediction is confirmed.} The gradient--responsibility identity holds to floating-point precision. LSE alone collapses as predicted. Variance prevents dead components; decorrelation prevents redundancy. The learned features are mixture components, digit prototypes rather than unstructured projections.

\paragraph{Training dynamics are consistent with EM structure.} SGD is insensitive to learning rate across three orders of magnitude. Lower loss does not correspond to better features. Adaptive optimizers offer no advantage. These observations were not predicted in advance but are consistent with implicit EM producing a well-conditioned optimization landscape.

\paragraph{The derived model performs well.} In our experiments, the theory-derived model achieves 93.4\% probe accuracy versus 90.3\% for a standard sparse autoencoder, with half the parameters and no decoder. Limited trials prevent strong claims, but that a model built purely from theory compares favorably to heuristic designs is itself evidence that the derivation is sound.

\subsection{Roadmap}
\label{sec:roadmap}

\Cref{sec:theory} develops the theoretical foundation: distance-based representations, the LSE identity, and volume control via InfoMax. \Cref{sec:model} instantiates this prescription as a concrete model. \Cref{sec:experiments} validates the theory's predictions experimentally. \Cref{sec:discussion} discusses the implications, including the role of decoders and what the optimization results reveal.