\section{Experimental Validation}
\label{sec:experiments}

We test each prediction in turn, then report an exploratory observation about training dynamics.

\subsection{Experiment 1: Theorem Verification}
\label{sec:exp-theorem}

\paragraph{Prediction.} The gradient equals the responsibility exactly (\Cref{eq:gradient-responsibility}).

\paragraph{Method.} We verify the identity with a single forward-backward pass. We generate random activations $a \in \mathbb{R}^{64 \times 128}$ (64 samples, 128 components), compute the LSE loss $L = -\sum_i \log \sum_j \exp(-a_{ij})$, compute responsibilities $r = \text{softmax}(-a)$, and backpropagate to obtain gradients. We then compare \texttt{a.grad} and $r$ element-wise across all 8,192 values.

No training is involved. The test isolates the mathematical identity from any learned parameters.

\paragraph{Results.} \Cref{fig:theorem} plots the gradient $\partial L_{\text{LSE}} / \partial a_j$ against the responsibility $r_j = \text{softmax}(-a)_j$ for all 8,192 values. All points lie on the $y=x$ line. The correlation is 1.0000; the maximum absolute error is $4.47 \times 10^{-8}$; the mean absolute error is $2.79 \times 10^{-9}$. These deviations are at floating-point precision.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.6\columnwidth]{fig1_theorem.pdf}
\caption{Gradient vs.\ responsibility for 8,192 random activations. All points lie on $y = x$, confirming the identity in \Cref{eq:gradient-responsibility} to floating-point precision.}
\label{fig:theorem}
\end{figure}

\paragraph{Interpretation.} Points on the y=x line indicate identical values. Prediction 1 is confirmed.

\subsection{Experiment 2: Ablation Study}
\label{sec:exp-ablation}

\paragraph{Predictions.} 
\begin{itemize}
    \item \emph{LSE only:} Complete collapse.
    \item \emph{LSE + variance:} No dead units, but redundancy.
    \item \emph{LSE + variance + decorrelation:} Stable, diverse representations.
    \item \emph{Variance + decorrelation only:} Viable representations but different dynamics.
\end{itemize}

\paragraph{Method.}
We train four configurations on MNIST with 64 components, 100 epochs, batch size 128, Adam at learning rate 0.001, and $\lambda_{\text{var}} = \lambda_{\text{tc}} = 1.0$ when enabled. Three random seeds per configuration (\Cref{tab:ablation-configs}).

\begin{table}[t]
\centering
\caption{Ablation configurations.}
\label{tab:ablation-configs}
\begin{tabular}{lccc}
\toprule
Configuration & LSE & Variance & Decorrelation \\
\midrule
LSE only & \checkmark & & \\
LSE + var & \checkmark & \checkmark & \\
LSE + var + tc & \checkmark & \checkmark & \checkmark \\
var + tc only & & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

We measure dead units (components with variance $< 0.01$), redundancy ($\|\text{Corr}(A) - I\|_F^2$), and responsibility entropy ($\mathbb{E}_x[H(r(x))]$, where higher values indicate softer competition).

\paragraph{Results.} \Cref{tab:ablation-results} summarizes the outcomes.

\begin{table}[t]
\centering
\caption{Ablation study results.}
\label{tab:ablation-results}
\begin{tabular}{lccc}
\toprule
Configuration & Dead Units & Redundancy & Resp.\ Entropy \\
\midrule
LSE only & 64/64 (100\%) & --- & 4.16 \\
LSE + var & 0/64 (0\%) & 1875 & 3.77 \\
LSE + var + tc & 0/64 (0\%) & 29 & 3.85 \\
var + tc only & 0/64 (0\%) & 28 & 1.99 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}

\emph{LSE only.} All 64 components died. The loss plateaued by epoch 10. Without volume control, collapse is complete.

\emph{LSE + variance.} No dead units, but redundancy explodes to 1875. Components are alive but nearly identical. Variance prevents collapse but not redundancy.

\emph{LSE + variance + decorrelation.} Adding decorrelation reduces redundancy by $64\times$ (from 1875 to 29). The full objective achieves zero dead units, low redundancy, and high responsibility entropy.

\emph{Variance + decorrelation only.} Without LSE, the model still achieves zero dead units and low redundancy. However, responsibility entropy drops from 3.85 to 1.99, indicating sharper competition. The LSE term provides soft responsibilities characteristic of mixture models. Without it, the model performs whitening rather than soft clustering.

Predictions 2â€“4 confirmed.

\subsection{Experiment 3: Benchmark Comparison}
\label{sec:exp-benchmark}

\paragraph{Goal.}
Assess whether the theory-derived model learns useful representations. We compare against a standard sparse autoencoder \citep{olshausen1996emergence,bricken2023monosemanticity}.

\paragraph{Method.}
We train two models on MNIST with matched hidden dimension, 100 epochs, batch size 128, Adam at learning rate 0.001, and five random seeds (\Cref{tab:benchmark-models}). The SAE uses L1 weight 0.01.

\begin{table}[t]
\centering
\caption{Model configurations for benchmark comparison.}
\label{tab:benchmark-models}
\begin{tabular}{llll}
\toprule
Model & Architecture & Loss & Parameters \\
\midrule
Theory-derived (ours) & Linear (784$\to$64) + ReLU & LSE + InfoMax & 50,240 \\
Standard SAE & Linear (784$\to$64) + ReLU + Linear (64$\to$784) & MSE + L1 & 101,200 \\
\bottomrule
\end{tabular}
\end{table}

We evaluate on four metrics:
\begin{itemize}
    \item \emph{Linear probe accuracy:} Freeze the encoder, train a linear classifier (sklearn LogisticRegression) on the features, report MNIST test accuracy.
    \item \emph{L0 sparsity:} Fraction of features active (nonzero) per input.
    \item \emph{Parameters:} Total trainable parameters.
    \item \emph{Reconstruction MSE:} For SAE, use the trained decoder. For our model, use $W^\top$ as a pseudo-decoder (no decoder is trained).
\end{itemize}

\paragraph{Results.} See \Cref{tab:benchmark-results}.

\begin{table}[t]
\centering
\caption{Benchmark comparison results.}
\label{tab:benchmark-results}
\begin{tabular}{lcc}
\toprule
Metric & Theory-Derived (Ours) & Standard SAE \\
\midrule
Linear Probe Accuracy & \textbf{93.43\% $\pm$ 0.38\%} & 90.26\% $\pm$ 0.32\% \\
L0 Density & \textbf{26.8\%} (17.2/64) & 50.3\% (32.2/64) \\
Parameters & \textbf{50,240} & 101,200 \\
Reconstruction MSE & 0.143 $\pm$ 0.001 & \textbf{0.026 $\pm$ 0.001} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Interpretation.}

\emph{Feature quality.}
The theory-derived model reaches 93.4\% linear probe accuracy versus 90.3\% for the SAE. For reference, logistic regression on raw MNIST pixels achieves approximately 92\%. The learned features outperform raw pixels; the SAE features underperform them.

\emph{Sparsity.}
Despite no explicit sparsity penalty, the theory-derived model activates only 27\% of features per input versus 50\% for the SAE trained with L1. Decorrelation induces sparsity indirectly: uncorrelated components must specialize on different inputs.

\emph{Parameters.}
Eliminating the decoder halves the parameter count.

\emph{Reconstruction.}
The SAE achieves lower reconstruction error. This is expected: it optimizes reconstruction with a trained decoder, whereas we use the untrained transpose $W^\top$. The probe results show that information is preserved, but encoded for discrimination rather than pixel-wise reconstruction.

The theory-derived model compares favorably to the SAE baseline: higher probe accuracy, sparser representations, and half the parameters. Both are baseline implementations without extensive tuning. Standard techniques (learning rate schedules, data augmentation, architectural refinements) could likely improve both. The point is not that our model beats an optimized SAE, but that a model derived purely from theory performs well without benchmark-driven tuning.

\subsection{Experiment 4: Feature Visualization}
\label{sec:exp-features}

\paragraph{Prediction.}
If the model performs implicit EM, learned features should resemble mixture components, prototypes that compete for data, rather than dictionary elements that combine additively \citep{olshausen1996emergence}. Encoder weights should show global structure (whole patterns) rather than local parts (edges or strokes).

\paragraph{Method.}
We visualize encoder weights from both models by reshaping each row of $W \in \mathbb{R}^{64 \times 784}$ into a $28 \times 28$ image. A diverging colormap shows positive weights (blue), negative weights (red), and zero (white). Each model is scaled to its own maximum absolute weight.

\paragraph{Results.}
\Cref{fig:features-ours,fig:features-sae} show the learned encoder weights for both models.

The theory-derived model (\Cref{fig:features-ours}) learns clear digit prototypes. Across the 64 features, multiple variants of each digit appear: circular 0s, vertical and slanted 1s, loopy and angular 2s, distinct forms of 3s through 9s. Many features exhibit center-surround structure, a digit-shaped region of one sign surrounded by the opposite sign, indicating that each component acts as both detector and suppressor. All features are active and interpretable; no dead units or degenerate patterns.

The standard SAE encoder (\Cref{fig:features-sae}) shows largely unstructured weights. Most features resemble low-magnitude noise, with only faint digit-like structure visible in a subset. The SAE encoder weights are qualitatively noisier and less organized.

\begin{figure}[!htbp]
\centering
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\textwidth]{fig4a_features.pdf}
\caption{Theory-derived model}
\label{fig:features-ours}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.48\columnwidth}
\centering
\includegraphics[width=\textwidth]{fig4b_sae_features.pdf}
\caption{Standard SAE}
\label{fig:features-sae}
\end{subfigure}
\caption{Learned encoder weights. (a) Theory-derived model: features form recognizable digit prototypes with center-surround structure, consistent with mixture components competing for data. (b) Standard SAE: encoder weights show little interpretable structure; representational content is carried primarily by the decoder.}
\label{fig:features}
\end{figure}

\paragraph{Interpretation.}
The theory-derived model learns prototypes because it is a mixture model. Each row of $W$ defines a component; the LSE objective induces soft competition; the InfoMax terms enforce distinctness. The weights resemble GMM centroids because that is what implicit EM produces.

The SAE encoder weights lack structure because the encoder need only produce activations the decoder can invert. Structure resides in the decoder.

This explains the probe accuracy gap in \Cref{sec:exp-benchmark}. Because encoder weights are organized by digit class, a linear probe effectively reads out class identity. A probe on SAE features must compose unstructured activations into class predictions, a harder task.

\subsection{Experiment 5: Training Dynamics}
\label{sec:exp-dynamics}

\paragraph{Motivation.}
We trained all previous experiments with Adam per standard practice. Out of curiosity, we tried SGD and were surprised by the results.

\paragraph{Method.}
We train the model using SGD and Adam across learning rates spanning three orders of magnitude ($10^{-4}$ to $10^{-1}$), with three random seeds per configuration (24 runs total). Other settings match \Cref{sec:exp-ablation}. We analyze loss trajectories and evaluate feature quality via linear probe accuracy.

\paragraph{Results.} See \Cref{tab:dynamics,fig:dynamics}.

\begin{table}[t]
\centering
\caption{Training dynamics results across optimizers and learning rates.}
\label{tab:dynamics}
\begin{tabular}{llcc}
\toprule
Optimizer & lr & Final Loss & Probe Acc \\
\midrule
SGD & 0.0001 & $-520 \pm 1$ & 92.2\% $\pm$ 0.3\% \\
SGD & 0.001 & $-676 \pm 0$ & 92.6\% $\pm$ 0.2\% \\
SGD & 0.01 & $-795 \pm 29$ & 93.6\% $\pm$ 0.1\% \\
SGD & 0.1 & $-967 \pm 18$ & 93.5\% $\pm$ 0.1\% \\
Adam & 0.0001 & $-745 \pm 2$ & 92.9\% $\pm$ 0.0\% \\
Adam & 0.001 & $-999 \pm 1$ & 93.5\% $\pm$ 0.4\% \\
Adam & 0.01 & $-1215 \pm 21$ & 93.5\% $\pm$ 0.2\% \\
Adam & 0.1 & $-1420 \pm 13$ & 93.1\% $\pm$ 0.2\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\columnwidth]{fig5_dynamics_loss.pdf}
\caption{Loss curves for SGD (left) and Adam (right) across learning rates. SGD curves plateau; Adam curves continue descending. Despite achieving substantially lower loss, Adam produces features of comparable quality.}
\label{fig:dynamics}
\end{figure}

\paragraph{Finding 1: Adam's usual advantage disappears.}
On most objectives, Adam outperforms SGD, especially at low learning rates. That pattern does not appear here. SGD at lr = $10^{-4}$, normally impractically slow, reaches a stable solution within 100 epochs. The adaptive machinery provides little benefit.

\paragraph{Finding 2: Lower loss does not imply better features.}
Adam achieves nearly 50\% lower loss than SGD at lr = 0.1. Yet probe accuracy is indistinguishable: 93.1\% versus 93.5\%. The objective admits directions that reduce loss without affecting downstream utility.

\paragraph{Finding 3: SGD is insensitive to learning rate.}
Across a $1000\times$ range, SGD yields probe accuracy between 92.2\% and 93.6\%. Learning rate affects where SGD settles, but not whether it finds good features.

\paragraph{Interpretation.}
These results point to a well-conditioned objective. Responsibility weighting normalizes gradient magnitudes across components. This is precisely what adaptive optimizers approximate. When the objective provides it directly, Adam has little to add.

The analogy to classical EM is suggestive. Explicit EM has no learning rate because the M-step computes optimal updates given current responsibilities. Implicit EM introduces a step size, but if the gradient direction is already correct, scaling changes speed rather than outcome.

The remaining anomaly, Adam lowering loss without improving features, suggests the objective contains degrees of freedom orthogonal to representation quality. Characterizing this null space remains open.

\subsection{Summary}
\label{sec:exp-summary}

\Cref{tab:validation-summary} summarizes the results.

\begin{table}[t]
\centering
\caption{Summary of experimental validation.}
\label{tab:validation-summary}
\begin{tabular}{lll}
\toprule
Prediction & Experiment & Result \\
\midrule
Gradient = responsibility & Theorem & Exact ($10^{-8}$ error) \\
LSE alone collapses & Ablation & 100\% dead units \\
Variance prevents death & Ablation & 0\% dead units \\
Decorrelation prevents redundancy & Ablation & $64\times$ reduction \\
Features are mixture components & Visualization & Digit prototypes \\
\midrule
\textbf{Observation} & & \\
Model compares favorably to SAE & Benchmark & +3.1\% probe accuracy \\
SGD learning-rate insensitive & Dynamics & 92--94\% across $1000\times$ \\
\bottomrule
\end{tabular}
\end{table}

All predictions confirmed.