\section{What Implicit EM Theory Requires}
\label{sec:theory}

This section develops the theoretical foundation from which our model is derived. The derivation proceeds in three steps.

\begin{enumerate}
    \item We adopt the distance-based interpretation of neural network outputs, which provides the geometric substrate for what follows (\Cref{sec:distance-representations}).
    \item We present the log-sum-exp identity. The gradient with respect to each component output equals its responsibility, allowing gradient descent to perform implicit EM (\Cref{sec:lse-identity}).
    \item Neural LSE objectives lack the volume control that prevents collapse in classical mixture models. We address this with InfoMax regularization, using variance and decorrelation penalties as the neural analogue of the log-determinant (\Cref{sec:volume-control,sec:solution}).
\end{enumerate}

The section concludes with a summary mapping each theoretical requirement to its implementation (\Cref{sec:theory-summary}).

The result is a complete specification. The theory requires distances, an LSE objective, and explicit volume control. \Cref{sec:model} instantiates this specification as a concrete model.

\subsection{Distance-Based Representations}
\label{sec:distance-representations}

The standard interpretation of neural networks treats outputs as confidences or scores where a high output indicates strong evidence for a hypothesis. This interpretation, while intuitive, obscures the geometric structure underlying what neural networks compute.

An alternative interpretation reframes outputs as distances from learned prototype regions \citep{oursland2024mahalanobis}. With ReLU, each node's prototype surface is the half-space where its output is zero. Under this view, a linear layer followed by an activation computes a deviation from a learned reference. The pre-activation $z_j = w_j^\top x + b_j$ measures how far the input lies from the decision boundary in a scaled Euclidean space.

The connection to Gaussian components is exact. The Mahalanobis distance along a principal direction $v$ with eigenvalue $\lambda$ from a mean $\mu$ is
\begin{equation}
d = |\lambda^{-1/2} v^\top (x - \mu)|
\end{equation}
which has the form $|Wx + b|$ for appropriate $W$ and $b$. The weight vector encodes both the principal direction and the precision along it. Standard ReLU networks compute this via the identity $|z| = \text{ReLU}(z) + \text{ReLU}(-z)$, decomposing signed distance into two half-space detectors.

Our model uses a single linear layer followed by ReLU, yielding the half-space distance structure described above.

The key shift is semantic. A neural network performs the same operations whether you view its output as a confidence or a distance. Probabilities are derived quantities, arising only after exponentiation and normalization transform distances into relative likelihoods.

Throughout this paper, we adopt the distance-based interpretation: \textbf{lower distance means better match}. This convention, called energy in energy-based models \citep{lecun2006tutorial}, is essential for the results that follow.

\subsection{The LSE Identity}
\label{sec:lse-identity}

Consider an encoder that maps an input $x$ to $K$ component distances $d_1(x), \ldots, d_K(x)$. Following \Cref{sec:distance-representations}, lower distance indicates a better match. Given these distances, define the log-sum-exp marginal objective:
\begin{equation}
\label{eq:lse}
L_{\text{LSE}}(x) = -\log \sum_{j=1}^{K} \exp(-d_j(x))
\end{equation}

This objective has a straightforward interpretation: it is minimized when at least one component assigns low distance to the input. It encodes the requirement that \emph{some} component must explain each data point. This is the same intuition underlying mixture models \citep{bishop2006pattern}.

The key property of this objective is an exact algebraic identity \citep{oursland2025implicit}. Taking the gradient with respect to any component distance gives:
\begin{equation}
\label{eq:gradient-responsibility}
\frac{\partial L_{\text{LSE}}}{\partial d_j}
= \frac{\exp(-d_j)}{\sum_{k=1}^{K} \exp(-d_k)}
= r_j
\end{equation}
where $r_j$ is the softmax responsibility, the posterior probability that component $j$ explains the input under the current distances.

In classical expectation--maximization \citep{dempster1977maximum}, the E-step computes responsibilities and the M-step updates parameters using responsibility-weighted statistics. These steps alternate explicitly. With an LSE objective, this separation dissolves:
\begin{itemize}
    \item The forward pass computes distances, which determine responsibilities implicitly through the softmax in \Cref{eq:gradient-responsibility}.
    \item The backward pass computes gradients, which by \Cref{eq:gradient-responsibility} are exactly those responsibilities.
    \item The optimizer step updates parameters, with each component receiving gradient signal proportional to its responsibility for the input.
\end{itemize}

Gradient descent on an LSE objective \emph{is} EM, performed continuously rather than in discrete alternating steps.

\subsection{The Volume Control Requirement}
\label{sec:volume-control}

The LSE identity (\Cref{eq:gradient-responsibility}) provides a mechanism for learning. Responsibility-weighted gradient updates implement implicit EM. But this mechanism does not specify what should be learned.

In supervised learning, labels complete the objective. Cross-entropy has LSE structure, and target labels define the desired outcome. Responsibilities are trained to match those labels.

In unsupervised learning, there are no labels. The LSE loss alone admits degenerate solutions.

\paragraph{The collapse problem.} Without volume control, the LSE loss admits degenerate solutions. One failure mode is distance collapse: the network maps all inputs to nearby points, trivializing responsibilities. Another is winner dominance: a component that captures slightly more mass receives stronger gradients, captures more mass still, and dominates entirely. This positive feedback is inherent to EM dynamics and is classically controlled by the log-determinant term.

The result in either case is a collapsed representation. Components either produce uniform output or a single component claims everything while others die. The encoder may satisfy the LSE loss, but the representation carries little information.

\paragraph{The mixture model precedent.} This failure mode is not unique to neural networks. Gaussian mixture models face the same problem, and their solution is instructive. The log-likelihood of a data point under a GMM component includes a log-determinant term:
\[
\log P(x \mid k) \propto
-\frac{1}{2}(x - \mu_k)^\top \Sigma_k^{-1}(x - \mu_k)
-\frac{1}{2}\log\det(\Sigma_k)
\]

The first term is the Mahalanobis distance. The second term, the log-determinant, penalizes components with small covariance \citep{bishop2006pattern}. Without it, a component can shrink to a point, placing arbitrarily high density on any data point it occupies. The log-determinant enforces volume. Each component must occupy a region of support, not a singularity.

\paragraph{The missing term.} Neural LSE objectives have no equivalent to the log-determinant. Without explicit volume control, nothing prevents collapse. \Cref{sec:solution} identifies the required correction.

\subsection{The Solution}
\label{sec:solution}

The log-determinant in Gaussian mixture models plays two distinct roles, each addressing a different failure mode.

\paragraph{The diagonal role.} For a covariance matrix with eigenvalues $\lambda_1, \ldots, \lambda_K$, the log-determinant is $\sum_j \log \lambda_j$. With uncorrelated components, this reduces to $\sum_j \log \text{Var}(A_j)$. As any variance approaches zero, this term diverges to negative infinity. A component cannot produce constant output without incurring unbounded penalty. The diagonal enforces existence. Every component must maintain non-zero variance.

\paragraph{The off-diagonal role.} The log-determinant also depends on correlations between components. If two components are perfectly correlated, the covariance matrix becomes singular and its determinant vanishes. More generally, correlations reduce the determinant below what uncorrelated components would achieve. The off-diagonal structure enforces diversity. Components cannot become redundant copies of one another.

InfoMax regularization follows the same structural pattern with variance and decorrelation penalties.

\paragraph{Variance penalty.} The variance penalty discourages low-variance components:
\[
L_{\text{var}} = -\sum_{j=1}^{K} \log \text{Var}(A_j)
\]

Under Gaussian assumptions, log-variance is proportional to differential entropy \citep{linsker1988self}. Maximizing this term encourages each component to carry information rather than remain constant. The logarithmic barrier ensures that collapse is forbidden, not merely discouraged. As $\text{Var}(A_j) \to 0$, the penalty diverges. This is exactly the diagonal of the log-determinant in the uncorrelated case.


\paragraph{Decorrelation penalty.} The decorrelation penalty discourages redundant components:
\[
L_{\text{tc}} = \|\text{Corr}(A) - I\|_F^2
\]

Decorrelation approximates statistical independence at the level of second-order statistics \citep{bell1995information}. The penalty is zero when components are uncorrelated and increases with off-diagonal correlations. Components that respond identically across inputs are penalized. This corresponds to the off-diagonal structure of the log-determinant.

\paragraph{Role equivalence.} The correspondence between GMM volume control and InfoMax regularization is summarized in \Cref{tab:role-equivalence}.

\begin{table}[t]
\centering
\caption{Correspondence between GMM volume control and InfoMax regularization.}
\label{tab:role-equivalence}
\begin{tabular}{lll}
\toprule
GMM term & Neural equivalent & Function \\
\midrule
$\log \det(\Sigma)$ (diagonal) & $\sum_j \log \text{Var}(A_j)$ & Prevent dead components \\
$\log \det(\Sigma)$ (off-diagonal) & $-\|\text{Corr}(A) - I\|_F^2$ & Prevent redundant components \\
\bottomrule
\end{tabular}
\end{table}


Implicit EM theory demands distance computation, an LSE loss, and explicit volume control. The first two were established in \Cref{sec:distance-representations,sec:lse-identity}. The variance and decorrelation penalties supply the third. \Cref{sec:model} assembles these elements into a concrete model.

\subsection{Summary}
\label{sec:theory-summary}

Each component of the model follows from a specific theoretical source, as shown in \Cref{tab:theory-components}.

\begin{table}[t]
\centering
\caption{Theoretical sources and implementations for each model component.}
\label{tab:theory-components}
\begin{tabular}{lll}
\toprule
Component & Theory Source & Implementation \\
\midrule
Distances & \citet{oursland2024mahalanobis} & Linear layer ($z = Wx + b$) \\
Activation & Distance interpretation & ReLU \\
EM structure & \citet{oursland2025implicit} & LSE loss (\Cref{eq:lse}) \\
Volume control (diagonal) & Log-determinant structure & Variance penalty \\
Volume control (off-diagonal) & Log-determinant structure & Decorrelation penalty \\
\bottomrule
\end{tabular}
\end{table}

The variance penalty prevents collapse. The decorrelation penalty prevents redundancy. We added nothing for empirical convenience. \Cref{sec:model} presents the full instantiation.