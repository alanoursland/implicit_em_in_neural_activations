\section{Discussion}
\label{sec:discussion}

All of our EM theory predictions were confirmed through experimental results. This section considers what that means.

\subsection{What This Validates}
\label{sec:what-validates}

Implicit EM theory is generative. We built what it required. The model works, and it works for the reasons the theory predicts.

The experiments confirm every prediction (\Cref{tab:validation-summary}).

The theory came first. The experiments tested whether the model it specifies actually works. It does. And not just in the binary sense—the qualitative behavior also matches. Soft competition via responsibilities, competitive coverage, emergent sparsity without L1. These weren't fitted; they were predicted.

\subsection{Why It Works}
\label{sec:on-performance}

The theory-derived model compared favorably to the baseline. Higher probe accuracy, sparser representations, half the parameters. We expected a working model, perhaps a worse one. So why did it perform well?

Standard sparse autoencoders are assembled from compensatory mechanisms. The decoder enforces information preservation because without it, the encoder could discard information. The L1 penalty enforces sparsity because without it, all features would activate densely. The reconstruction loss anchors features to input fidelity because without it, representations could drift arbitrarily. Each component exists to counteract a failure mode that would otherwise emerge.

The result is a system at war with itself. Reconstruction wants features that combine to reproduce the input. L1 wants features that do not activate. The encoder wants to distribute information across features. The decoder wants to invert whatever the encoder produces. Training navigates these conflicting pressures, settling into a compromise that satisfies none of them fully.

Our model has no such conflicts. The LSE term attracts components toward data. The variance penalty prevents collapse. The decorrelation penalty prevents redundancy. Each term does one thing. The terms do not fight each other.

This may explain the performance gap. When the objective is internally coherent, optimization is straightforward. When the objective is a collection of patches, optimization is a negotiation. The SAE's encoder learns to produce something the decoder can invert while the L1 penalty tries to shut it down. Our encoder just learns prototypes.

A caveat: both models are baseline implementations. The SAE uses untied weights and standard L1 regularization. Many improvements exist. The theory-derived model uses the simplest architecture that satisfies the theoretical requirements. The point is not that principled derivation always beats heuristic engineering. The point is that it can, and when it does, the reason is coherence.

\subsection{Reconstruction and Volume Control}
\label{sec:reconstruction}

The decoder has been a fixture of autoencoder architectures since their introduction \citep{hinton2006reducing}. Sparse autoencoders inherited this structure, adding L1 penalties while retaining the encoder-decoder-reconstruction framework \citep{vincent2008extracting}. The decoder appeared necessary. Remove it, and representations degenerate.

Why did representations degenerate? The standard explanation is that reconstruction enforces information preservation. Without pressure to reproduce the input, the encoder could discard information arbitrarily. This explanation is correct but incomplete. It describes what the decoder does without asking whether reconstruction is the only way to achieve it.

Reconstruction implicitly supplies volume control.

\paragraph{Anti-collapse.} If a feature never activates, it contributes nothing to reconstruction. Inputs that need that feature incur higher reconstruction error, producing gradient signal that revives the dead unit. Reconstruction prevents collapse because dead features degrade reconstruction. This is the diagonal of the log-determinant: components must have non-zero variance.

\paragraph{Anti-redundancy.} If two features encode identical information, the decoder can use either one. Using both offers no benefit. The L1 penalty then suppresses the redundant copy, since it incurs sparsity cost without improving reconstruction. Reconstruction combined with L1 discourages redundancy because redundant features are wasteful. This is the off-diagonal of the log-determinant: components must be decorrelated.

The decoder provides volume control indirectly, through the reconstruction bottleneck. We provide it directly, through explicit regularization. Same function, different mechanism.

With explicit volume control, the decoder becomes unnecessary. The variance penalty prevents collapse without relying on reconstruction to revive dead units. The decorrelation penalty prevents redundancy without relying on L1 to prune duplicates. The functions the decoder served are now enforced by the objective itself.

And something interesting happens. Properties that SAEs achieve through opposing forces emerge naturally from coherent ones.

Sparsity, for instance. SAEs achieve sparsity through L1, which penalizes activation. The objective wants features to activate for reconstruction; L1 wants them to stay off. Sparsity emerges from this fight. In our model, sparsity emerges from specialization. When components carve up the input space, most inputs fall in the territory of only a few. The others simply are not relevant. Sparsity as consequence of structure.

Information preservation, similarly. SAEs verify that information survives by demanding reconstruction. If the decoder cannot reproduce the input, something was lost. Our model ensures information survives directly. Variance means each feature carries information. Decorrelation means features carry different information. No decoder needed to check.

Feature visualization makes this concrete (\Cref{sec:exp-features}). Our encoder weights are interpretable prototypes. Each row of $W$ forms a recognizable digit pattern with center-surround structure. The SAE encoder weights are largely unstructured, with only faint patterns visible under close inspection.

This asymmetry is revealing. In the SAE, the decoder carries the representational burden. The encoder merely produces signals the decoder can invert. Interpretability is not required of the encoder because structure resides in the decoder. In our model, the encoder is the representation. There is no decoder to compensate for an unstructured projection. The encoder must learn structure, and it does.

The decoder was compensatory. It patched a deficiency in the objective by supplying volume control implicitly through reconstruction. The log-determinant's functions, smuggled in through the back door. Once the deficiency is addressed directly, the patch is unnecessary. The decoder was never about reconstruction. It was about preventing collapse and redundancy. Those functions now reside where they belong.

\subsection{Connection to Self-Supervised Learning}
\label{sec:self-supervised}

The goals of our InfoMax terms appear in recent self-supervised methods, discovered independently through extensive empirical research. Barlow Twins \citep{zbontar2021barlow} and VICReg \citep{bardes2022vicreg} both prevent collapse and redundancy through explicit regularization.

VICReg makes the structure particularly clear: variance, invariance, covariance. The variance term keeps feature standard deviations above a threshold. The covariance term penalizes off-diagonal entries of the covariance matrix. Same goals as our variance and decorrelation penalties, different implementations. Where they use invariance to augmentations as the primary objective, we use LSE attraction.

This convergence from different directions is striking. The self-supervised community arrived at these constraints through years of careful experimentation on large-scale vision tasks, systematically identifying what prevents representation collapse. We arrived at them through analogy to the log-determinant in mixture models. That both paths lead to variance and decorrelation suggests this structure is fundamental.

The implicit EM framework offers a theoretical interpretation for their empirical findings: these regularizers enforce volume control. Conversely, the success of Barlow Twins and VICReg at scale provides evidence that our derivation, validated here only on MNIST, rests on solid foundations. Their work suggests the approach scales. Our work suggests why it works.

\subsection{On Optimization}
\label{sec:on-optimization}

The training dynamics experiment revealed something odd. SGD plateaus by roughly epoch 70 regardless of learning rate. Adam keeps descending, reaching nearly 50\% lower loss, but feature quality stays the same. What is going on?

Classical EM has no learning rate. The algorithm alternates between computing responsibilities and updating parameters to maximize expected log-likelihood. Step size is fixed by the mathematics. Convergence occurs when responsibilities stabilize, in a number of iterations determined by the problem.

Implicit EM may inherit this behavior. The gradient of the LSE objective equals the responsibility. SGD follows this gradient directly. When responsibilities stabilize, gradients vanish and SGD stops. Learning rate scales how fast you get there, but not when you arrive. This would explain why SGD at $10^{-4}$ and SGD at $10^{-1}$ reach similar feature quality: they trace the same path at different speeds.

Adam complicates the picture. Its adaptive per-parameter scaling is designed for ill-conditioned objectives. But if responsibility weighting already normalizes gradients across components, the landscape is well-conditioned by construction. Adam's machinery becomes unnecessary. Worse, near equilibrium its second-moment estimate shrinks as gradients become small, effectively increasing step size when it should decrease. The optimizer keeps pushing when it should stop.

The puzzle is what Adam is optimizing. Loss keeps dropping. Features do not improve. This suggests the objective has degrees of freedom orthogonal to representation quality. Directions in parameter space that reduce loss without changing what the model learns. What are they? We do not know. Characterizing this null space remains open.

A caveat: this is speculation. The observations are consistent with implicit EM producing a well-conditioned landscape where simple optimizers suffice. Responsibility structure and learning-rate invariance co-occur; whether one causes the other remains open. The connection to classical EM is suggestive.

\subsection{Limitations}
\label{sec:limitations}

This work validates implicit EM theory on a minimal test case.

\paragraph{Scale.} The model is a single linear layer with 64 components, trained on MNIST. The theory holds for any differentiable parameterization of distances, but we have not tested deeper architectures, larger datasets, or hundreds of components. The competitive dynamics that work here may behave differently at scale.

\paragraph{LLM activations untested.} A primary motivation for sparse autoencoders is interpretability of large language models \citep{bricken2023monosemanticity,cunningham2023sparse}. We tested on images. The features learned on MNIST are digit prototypes, visually interpretable. Whether similar structure emerges for linguistic or abstract features is an open question.

\paragraph{Formal EM connection incomplete.} We call this implicit EM based on the gradient-responsibility identity. But we have not derived closed-form M-step updates, proved convergence guarantees, or established the precise relationship between gradient descent and EM fixed-point iterations. The interpretation remains empirical.

\paragraph{Hyperparameters not tuned.} We used $\lambda_{\text{var}} = \lambda_{\text{tc}} = 1.0$ throughout with no systematic search. The SAE baseline used L1 weight 0.01 without tuning. Different settings might narrow or widen the performance gap. Our goal was to validate the theory.

We tested the theory in one setting. Its scope may be broader. Deeper networks, larger scale, language model activations, and formal EM connections remain for future work.

\subsection{Future Directions}
\label{sec:future}

This theory suggests several paths to explore.

\paragraph{Explicit EM.} Can we derive closed-form M-steps for the linear case? Gradient descent works, but explicit EM might converge faster, require no learning rate, and come with guarantees.

\paragraph{Supervised regularization.} Cross-entropy already has LSE structure. Adding InfoMax during supervised learning would encourage mixture-like representations even with labels. Structured features alongside discriminative ones.

\paragraph{Layer-wise pretraining.} Hinton and others showed this works \citep{hinton2006reducing,bengio2006greedy}. The field moved on when end-to-end backprop proved sufficient. But reconstruction was always an awkward objective for intermediate layers. LSE+InfoMax is principled. Each layer learns a mixture model over its inputs. Hierarchical mixtures, all the way up.

\paragraph{Conditioning pretrained models.} Apply LSE+InfoMax to representations learned by supervised training. How does the unsupervised structure differ? Fine-tune and measure how far supervision pushes from the mixture solution.

\paragraph{Activation geometry.} ReLU gives half-space prototypes, with zero distance for an entire region. Softplus gives smooth, always-positive distance. What happens to mixture component character when the geometry changes? Do sharp boundaries produce sharper prototypes? Does smoothness help or hurt specialization? A systematic study across activations could reveal whether the choice matters more than we assumed.

\paragraph{Attention.} Prior work established that attention is implicit EM—softmax over key similarities produces responsibilities, and value vectors receive responsibility-weighted updates \citep{oursland2025implicit}. The natural question is whether InfoMax regularization transfers. Decorrelation across attention heads could address the known redundancy problem where heads learn similar patterns. Variance penalties could prevent head collapse. The same volume control that stabilizes mixture components might stabilize attention.

\paragraph{Anomaly detection.} High LSE loss means no component explains the input well. This is a natural anomaly score, available without modification. Does it outperform reconstruction error? Does it catch different failure modes? The framework hands us an anomaly detector for free—someone should test whether it works.

\paragraph{Optimal transport.} Responsibilities distribute inputs across components. Optimal transport distributes mass across sinks. The LSE objective may be minimizing a transport cost we have not identified. If so, OT convergence guarantees might transfer, and Sinkhorn-like algorithms might accelerate training.

\paragraph{Interpretability.} Apply the framework post-hoc to trained models. Compute responsibilities for existing representations. Do pretrained vision models already have mixture structure hiding in their activations? If so, the framework becomes a lens for understanding what networks have already learned.

We have validated the theory in one setting. These directions suggest it may reach further.


