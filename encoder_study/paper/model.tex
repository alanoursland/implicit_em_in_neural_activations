\section{The Derived Model}
\label{sec:model}

We first present the architecture (\Cref{sec:architecture}), then define the complete objective (\Cref{sec:objective}). We analyze the dynamics that emerge from the objective (\Cref{sec:dynamics}) and state the theoretical predictions that \Cref{sec:experiments} tests (\Cref{sec:predictions}).

\subsection{Architecture}
\label{sec:architecture}

The identity in \Cref{eq:gradient-responsibility} holds for any differentiable parameterization of the distances $d_j(x)$ \citep{oursland2025implicit}. We therefore adopt the simplest instantiation: a single linear layer followed by a nonlinearity.
\begin{equation}
\label{eq:architecture}
z = Wx + b, \qquad d = \phi(z)
\end{equation}

Here $W \in \mathbb{R}^{K \times D}$ maps inputs to $K$ components. 

We use ReLU in our experiments. Other activations can be substituted without affecting the implicit EM property. ReLU produces non-negative distances, with zero indicating a strong match.

Responsibilities need not be computed explicitly during training. By \Cref{eq:gradient-responsibility}, they appear in the gradients. For analysis, they can be recovered as
\begin{equation}
\label{eq:responsibilities}
r = \text{softmax}(-d)
\end{equation}

This is unsupervised learning. We do not reconstruct the input. There is no decoder.

\subsection{Complete Objective}
\label{sec:objective}

The objective from \Cref{eq:objective-intro} expands to
\begin{equation}
\label{eq:full-objective}
L = -\log \sum_{j=1}^{K} \exp(-d_j) - \lambda_{\text{var}} \sum_{j=1}^{K} \log \text{Var}(d_j) + \lambda_{\text{tc}} \|\text{Corr}(d) - I\|_F^2
\end{equation}

Variance and correlation are computed across batches during training.

\paragraph{LSE term.}
At least one component must explain each input. The LSE term attracts components toward data they explain well.

\paragraph{Variance penalty.}
Components must maintain non-zero variance. The hyperparameter $\lambda_{\text{var}}$ controls the strength of this constraint.

\paragraph{Decorrelation penalty.}
Prevents redundancy. Controlled by $\lambda_{\text{tc}}$.

For some architectures, a weight regularizer can help:
\begin{equation}
\label{eq:weight-reg}
L_{\text{wr}} = \lambda_{\text{wr}} \|W^\top W - I\|_F^2
\end{equation}

This encourages orthogonality among weight vectors, preventing multiple components from converging to the same direction. We do not use it in our experiments.

\subsection{What This Model Is}
\label{sec:what-model-is}

The architecture in \Cref{eq:architecture} and the objective in \Cref{eq:full-objective} admit three complementary descriptions, each illuminating a different aspect of the same system.

\paragraph{A decoder-free sparse autoencoder.}
Standard SAEs map inputs through an encoder to a sparse bottleneck, then reconstruct via a decoder, training on reconstruction loss plus an L1 sparsity penalty \citep{olshausen1996emergence,bricken2023monosemanticity}. Our model retains only the encoder with no explicit sparsity penalty. It learns sparse, interpretable features comparable to those of standard SAEs (\Cref{sec:experiments}).

\paragraph{A neural mixture model.}
Each row of $W$ defines a component. The distances $d_j(x)$ measure how well each component explains the input. The LSE objective is the negative log marginal likelihood. The InfoMax terms play the role of the log-determinant. This is not an analogy. The mathematics are identical; only the parameterization differs.

\paragraph{A minimal test case.}
The model exists to test the theory. \Cref{eq:architecture} is the simplest architecture that computes distances. \Cref{eq:full-objective} is the objective the theory specifies. We add no architectural embellishments, auxiliary losses, or heuristics.

\subsection{Dynamics}
\label{sec:dynamics}

The objective balances attraction against structure. This explains why the model learns useful representations rather than collapsing.

\subsubsection{The LSE Term Is Attractive}

The LSE term pulls components toward data they explain well. The gradient identity ensures this attraction is responsibility-weighted. Left unchecked, this leads to collapse (\Cref{sec:volume-control}).

\subsubsection{The InfoMax Terms Are Structural}

The variance and decorrelation penalties constrain \emph{how} components can reduce their distance, without dictating \emph{where} they should go. Variance enforces selectivity. Decorrelation enforces diversity. Together, they shape attraction into a stable equilibrium.

\subsubsection{The Equilibrium Is Competitive Coverage}

Components distribute themselves to tile the data manifold, each specializing in a region of input space. A component cannot expand its territory without reducing its variance or overlapping with others, both of which are penalized. The result is a soft partition of the input space.

This resembles competitive learning in self-organizing maps \citep{kohonen1982self}, but with soft responsibilities rather than winner-take-all assignments.

\subsubsection{Sparsity Is Emergent}

The objective contains no explicit sparsity penalty. Sparse representations nevertheless arise. As components specialize, each input activates only a few components. Sparsity emerges from structure, not from L1 regularization.

\subsection{Theoretical Predictions}
\label{sec:predictions}

The theory makes five testable predictions.

\begin{table}[t]
\centering
\caption{Theoretical predictions and their sources.}
\label{tab:predictions}
\begin{tabular}{ll}
\toprule
Prediction & Theoretical Source \\
\midrule
Gradient equals responsibility exactly & LSE identity (\Cref{eq:gradient-responsibility}) \\
LSE alone collapses & Missing volume control (\Cref{sec:volume-control}) \\
Variance term prevents dead units & Diagonal of log-determinant (\Cref{sec:solution}) \\
Decorrelation prevents redundancy & Off-diagonal of log-determinant (\Cref{sec:solution}) \\
Features are mixture components & Implicit EM interpretation (\Cref{sec:lse-identity}) \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Prediction 1: Gradient equals responsibility.}
\Cref{eq:gradient-responsibility} is an algebraic identity. It should hold to numerical precision.

\paragraph{Prediction 2: LSE alone collapses.}
Without volume control, all components should die.

\paragraph{Prediction 3: Variance prevents collapse.}
Adding variance penalty should prevent dead components. Without decorrelation, components may still be redundant.

\paragraph{Prediction 4: Decorrelation prevents redundancy.}
Adding decorrelation should force components to encode distinct information.

\paragraph{Prediction 5: Features are mixture components.}
Learned features should resemble prototypes that compete for data, not dictionary elements that combine additively. Visualized features should exhibit global structure rather than local parts.

Confirming all five would support implicit EM theory as a foundation for model design.