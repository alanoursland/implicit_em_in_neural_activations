"""Generate paper figures from experiment results.

Outputs:
- figures/fig1_theorem.pdf — Gradient vs responsibility scatter
- figures/fig2_ablation.pdf — Similarity matrices or bar chart
- figures/fig3_benchmark.pdf — Comparison table/chart
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

import json
import argparse
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


def generate_theorem_figure(output_path: str):
    """Generate Figure 1: Theorem verification.

    Note: This is usually generated by verify_theorem.py directly.
    This function can regenerate it from cached data if available.
    """
    # This is typically generated by verify_theorem.py
    # Here we just create a placeholder or regenerate
    from scripts.verify_theorem import verify_theorem

    verify_theorem(output_path=output_path)
    print(f"Figure 1 saved to {output_path}")


def generate_ablation_figure(results_path: str, output_path: str):
    """Generate Figure 2: Ablation results.

    Creates a 2x2 grid showing feature similarity matrices for each config,
    plus a summary bar chart.
    """
    # Load results
    with open(results_path, "r") as f:
        results = json.load(f)

    # Group by config (take first seed for similarity matrices)
    configs = {}
    for r in results:
        name = r["config_name"]
        if name not in configs:
            configs[name] = r

    # Create figure with subplots
    fig = plt.figure(figsize=(14, 10))

    # Top row: Feature similarity matrices (2x2)
    config_order = ["lse_only", "lse_var", "lse_var_tc", "var_tc_only"]
    config_labels = {
        "lse_only": "A: LSE only",
        "lse_var": "B: LSE + Var",
        "lse_var_tc": "C: LSE + Var + TC",
        "var_tc_only": "D: Var + TC only",
    }

    for i, cfg_name in enumerate(config_order):
        if cfg_name not in configs:
            continue

        ax = fig.add_subplot(2, 3, i + 1)
        sim_matrix = np.array(configs[cfg_name]["feature_similarity"])

        # Plot heatmap
        im = ax.imshow(sim_matrix, cmap="RdBu_r", vmin=-1, vmax=1)
        ax.set_title(config_labels.get(cfg_name, cfg_name), fontsize=11)
        ax.set_xlabel("Feature j")
        ax.set_ylabel("Feature i")

    # Add colorbar
    cbar_ax = fig.add_axes([0.68, 0.55, 0.02, 0.35])
    fig.colorbar(im, cax=cbar_ax, label="Cosine Similarity")

    # Bottom row: Summary metrics bar chart
    ax_metrics = fig.add_subplot(2, 1, 2)

    # Aggregate metrics across seeds
    metric_data = {"config": [], "dead_units": [], "redundancy": [], "mse": []}

    for r in results:
        metric_data["config"].append(config_labels.get(r["config_name"], r["config_name"]))
        metric_data["dead_units"].append(r["final_metrics"]["dead_units"])
        metric_data["redundancy"].append(r["final_metrics"]["redundancy_score"])
        metric_data["mse"].append(r["final_metrics"]["reconstruction_mse"])

    # Compute means and stds per config
    unique_configs = list(config_labels.values())
    x = np.arange(len(unique_configs))
    width = 0.25

    dead_means = []
    dead_stds = []
    redund_means = []
    redund_stds = []

    for cfg_label in unique_configs:
        indices = [i for i, c in enumerate(metric_data["config"]) if c == cfg_label]
        if indices:
            dead_means.append(np.mean([metric_data["dead_units"][i] for i in indices]))
            dead_stds.append(np.std([metric_data["dead_units"][i] for i in indices]))
            redund_means.append(np.mean([metric_data["redundancy"][i] for i in indices]))
            redund_stds.append(np.std([metric_data["redundancy"][i] for i in indices]))
        else:
            dead_means.append(0)
            dead_stds.append(0)
            redund_means.append(0)
            redund_stds.append(0)

    # Normalize redundancy for visualization
    redund_means_norm = np.array(redund_means) / max(max(redund_means), 1) * 64

    bars1 = ax_metrics.bar(x - width/2, dead_means, width, yerr=dead_stds,
                           label="Dead Units", color="coral", capsize=3)
    bars2 = ax_metrics.bar(x + width/2, redund_means_norm, width, yerr=np.array(redund_stds)/max(max(redund_means), 1)*64,
                           label="Redundancy (scaled)", color="steelblue", capsize=3)

    ax_metrics.set_xlabel("Configuration")
    ax_metrics.set_ylabel("Count / Score")
    ax_metrics.set_title("Ablation Metrics: Dead Units and Feature Redundancy")
    ax_metrics.set_xticks(x)
    ax_metrics.set_xticklabels(unique_configs)
    ax_metrics.legend()
    ax_metrics.grid(axis="y", alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Figure 2 saved to {output_path}")


def generate_benchmark_figure(results_path: str, output_path: str):
    """Generate Figure 3: Benchmark comparison.

    Creates a comparison table/chart showing MSE, sparsity, and parameters.
    """
    # Load results
    with open(results_path, "r") as f:
        results = json.load(f)

    ours = results["ours"]
    baseline = results["baseline"]

    # Compute statistics
    ours_mse = [r["reconstruction_mse"] for r in ours]
    ours_l0 = [r["sparsity_l0"] for r in ours]
    ours_params = ours[0]["parameters"]

    baseline_mse = [r["reconstruction_mse"] for r in baseline]
    baseline_l0 = [r["sparsity_l0"] for r in baseline]
    baseline_params = baseline[0]["parameters"]

    # Create figure
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))

    # Bar positions
    x = np.array([0, 1])
    labels = ["Ours\n(LSE + InfoMax)", "Baseline\n(SAE + L1)"]

    # Plot 1: Reconstruction MSE
    ax = axes[0]
    means = [np.mean(ours_mse), np.mean(baseline_mse)]
    stds = [np.std(ours_mse), np.std(baseline_mse)]
    bars = ax.bar(x, means, yerr=stds, capsize=5, color=["steelblue", "coral"])
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.set_ylabel("Reconstruction MSE")
    ax.set_title("Reconstruction Quality")
    ax.grid(axis="y", alpha=0.3)

    # Add value labels
    for i, (m, s) in enumerate(zip(means, stds)):
        ax.text(i, m + s + 0.001, f"{m:.4f}", ha="center", va="bottom", fontsize=10)

    # Plot 2: Sparsity (L0)
    ax = axes[1]
    means = [np.mean(ours_l0), np.mean(baseline_l0)]
    stds = [np.std(ours_l0), np.std(baseline_l0)]
    bars = ax.bar(x, means, yerr=stds, capsize=5, color=["steelblue", "coral"])
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.set_ylabel("Active Units (L0)")
    ax.set_title("Sparsity")
    ax.grid(axis="y", alpha=0.3)

    for i, (m, s) in enumerate(zip(means, stds)):
        ax.text(i, m + s + 0.5, f"{m:.1f}", ha="center", va="bottom", fontsize=10)

    # Plot 3: Parameters
    ax = axes[2]
    params = [ours_params, baseline_params]
    bars = ax.bar(x, params, color=["steelblue", "coral"])
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.set_ylabel("Parameters")
    ax.set_title("Model Size")
    ax.grid(axis="y", alpha=0.3)

    for i, p in enumerate(params):
        ax.text(i, p + 1000, f"{p:,}", ha="center", va="bottom", fontsize=10)

    # Add savings annotation
    savings = (baseline_params - ours_params) / baseline_params * 100
    ax.annotate(
        f"{savings:.0f}% fewer",
        xy=(0, ours_params),
        xytext=(0.5, (ours_params + baseline_params) / 2),
        ha="center",
        fontsize=10,
        color="green",
        arrowprops=dict(arrowstyle="->", color="green"),
    )

    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"Figure 3 saved to {output_path}")


def main():
    parser = argparse.ArgumentParser(description="Generate paper figures")
    parser.add_argument(
        "--results-dir",
        type=str,
        default="results",
        help="Directory containing experiment results",
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="figures",
        help="Directory to save figures",
    )
    parser.add_argument(
        "--figure",
        type=str,
        choices=["all", "1", "2", "3"],
        default="all",
        help="Which figure to generate",
    )
    args = parser.parse_args()

    results_dir = Path(args.results_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    if args.figure in ["all", "1"]:
        generate_theorem_figure(str(output_dir / "fig1_theorem.pdf"))

    if args.figure in ["all", "2"]:
        ablation_results = results_dir / "ablation" / "ablation_results.json"
        if ablation_results.exists():
            generate_ablation_figure(
                str(ablation_results),
                str(output_dir / "fig2_ablation.pdf"),
            )
        else:
            print(f"Warning: {ablation_results} not found. Run ablation first.")

    if args.figure in ["all", "3"]:
        benchmark_results = results_dir / "benchmark" / "benchmark_results.json"
        if benchmark_results.exists():
            generate_benchmark_figure(
                str(benchmark_results),
                str(output_dir / "fig3_benchmark.pdf"),
            )
        else:
            print(f"Warning: {benchmark_results} not found. Run benchmark first.")


if __name__ == "__main__":
    main()
